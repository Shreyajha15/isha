---
title: "Assignment-3"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
library(ggplot2)
library(clValid)
library(cluster)
#library(fpc)

df = read.csv('FBS.csv')
head(df, 5)

# Check if any missing values exist in any columns
if(any(is.na(df[, "StadiumCapacity"]))){
  # Handle Missing Values in StadiumCapacity
  message("StadiumCapacity has missing values")
}

# Check if any missing values exist in any columns
if(any(is.na(df[, "Latitude"]))){
  # Handle Missing Values in StadiumCapacity
  #df$Latitude
  message("Latitude has missing values")
}

# Check if any missing values exist in any columns
if(any(is.na(df[, "Longitude"]))){
  # Handle Missing Values in StadiumCapacity
  message("Longitude has missing values")

}

# Check if any missing values exist in any columns
if(any(is.na(df[, "AthleticRevenue...."]))){
  # Handle Missing Values in StadiumCapacity
  message("AthleticRevenue.... has missing values")
  ggplot(data = df, mapping = aes(x = AthleticRevenue....)) +
  geom_histogram(na.rm = TRUE, binwidth = 5.0e+07)
  
  medianVal = median(df$AthleticRevenue...., na.rm = TRUE)
  df$AthleticRevenue....[is.na(df$AthleticRevenue....)] = medianVal
  
  message("Any there any more NA values: ", any(is.na(df)))
}

# Check if any missing values exist in any columns
if(any(is.na(df[, "Endowment...000."]))){
  # Handle Missing Values in StadiumCapacity
  #df$Endowment...000.
  message("Endowment...000. has missing values")

}

# Check if any missing values exist in any columns
if(any(is.na(df[, "Enrollment"]))){
  # Handle Missing Values in StadiumCapacity
  #df$Enrollment
  message("Enrollment has missing values")
}

```
Remove the non-numeric Column
```{r}
# Since grouping by school nameis not relevant removing it

# Remove the school Column. That means, cluster based-on StadiumCapacity, Latitude, Logitude, AthleticRevenue, Endowment and enrollment columns.

colNames = colnames(df)[c(-1)]
df_num = subset.data.frame(x = df, select = colNames, drop = FALSE)
colnames(df_num)

```
```{r}

distance = dist(df_num, method="euclidean")

```

Data exploring through graphs
```{r}

ggplot(data = df_num, mapping = aes(x = Longitude, y = Latitude, col = StadiumCapacity)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm")

# Observations from graph:
# All the data except one is concentrated between Longitude(-125, -70) and latitude(25, 50) - One entry is located at Logitude -155 & latitude 19

```

```{r}

ggplot(data = df_num, mapping = aes(x = StadiumCapacity, y = AthleticRevenue....)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm")

# Observations from graph:
# Stadium Capacity and Athletic Revenue have positive linear relationship

```

```{r}

ggplot(data = df_num, mapping = aes(x = StadiumCapacity, y = AthleticRevenue....)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 16,
             outlier.size = 2, notch = FALSE)
```
```{r}

ggplot(data = df_num, mapping = aes(x = StadiumCapacity)) +
  geom_histogram(stat = "bin", bins = 40)

# Observations from graph:
# StadiumCapacity data distribution is Multi-Modal and Right-Skewed

```

```{r}

ggplot(data = df_num, mapping = aes(x = AthleticRevenue....)) +
  geom_histogram(stat = "bin", bins = 40)

# Observations from graph:
# AthleticRevenue.... data distribution is Multi-Modal and Right-Skewed

```

```{r}

ggplot(data = df_num, mapping = aes(x = Endowment...000.)) +
  geom_histogram(stat = "bin", bins = 40)

# Observations from graph:
# Endowment...000. data distribution is Right-Skewed

```

Without Normalization

Hierarchical Clustering using Ward method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

# Depending on the information we got from data exploration, initial thoughts is that it is best to use ward/ average/ centroid/ complete linkages to compute the clusters. Of all these, ward method ensures that similar records are clustered such that loss of information after clustering is relatively least.

# To validate the cluster quality, dunn's index can be computed which suggests the dissimilarity between clusters.

# Dunn's index validates based on: small variance between members of the cluster, and well separated clusters, where the means of different clusters are sufficiently far apart, as compared to the within cluster variance.

# Higher the dunn's index, better the clustering

hClusters <- hclust(distance, method = "ward.D2")
plot(hClusters)
```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 4)
plot(groups)

clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# dunn's index: 0.058301

```
Hierarchical Clustering using Average method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "average")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 4)
plot(groups)


clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.07087089, higher than Ward method's Dunn's Index with total 4 clusters - 3 dense clusters and 1 sparse cluster, hence Average method clustering better.
```
Hierarchical Clustering using Centroid method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "centroid")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 2)
plot(groups)

clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.08737614, higher than average method's Dunn's Index with total 2 clusters - 1 overly dense cluster and 1 sparse cluster.

# Hence average method is better so far.

```
Hierarchical Clustering using single method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "single")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 2)
plot(groups)


clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.1479173, higher than centroid method's Dunn's Index with almost 1 cluster with all the records, hence so far average  method is clustering better

```
Hierarchical Clustering using complete method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "complete")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 4)
plot(groups)


clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.0280108, less than all the previous methods. So far, average method is found to be doing better clustering

```

Hierarchical Clustering using complete method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "median")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 4)
plot(groups)
clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.02527797, less than the previous methods. So far, average method is found to be doing better clustering

```

Hierarchical Clustering using mcquitty method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "mcquitty")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 4)
plot(groups)
clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.02527797, less than the previous methods. 

# Hence, average method is producing best clusters compared to all the others

```

Normalize Data
```{r}
# Normalize the data
df.norm <- data.frame(sapply(X = df_num, FUN = scale))
head(df.norm)

distance  = dist(df.norm, method = 'euclidean')
distance
  
```

With Normalization

Hierarchical Clustering using Ward method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

# Depending on the information we got from data exploration, initial thoughts is that it is best to use ward/ average/ centroid/ complete linkages to compute the clusters. Of all these, ward method ensures that loss of information is least after clustering. Hence choosing ward method.

hClusters <- hclust(distance, method = "ward.D2")
plot(hClusters)
```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 4)
plot(groups)

clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.1324915

```
Hierarchical Clustering using Average method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "average")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 5)
plot(groups)


clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.3447668, higher than Ward method's Dunn's Index with total 5 clusters - 1 overly-dense cluster and 4 single record clusters - which is not a good clustering.

# Hence Ward method clustering better.
```
Hierarchical Clustering using Centroid method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "centroid")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 6)
plot(groups)

clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.2342759, higher than Ward method's Dunn's Index with total 6 clusters - 1 overly-dense cluster and 5 single record clusters - which is not a good clustering.

# Hence Ward method clustering better.

```
Hierarchical Clustering using single method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "single")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 5)
plot(groups)


clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.3447668, higher than Ward method's Dunn's Index with total 5 clusters - 1 overly-dense cluster and 4 single record clusters - which is not a good clustering.

# Hence Ward method clustering better.

```
Hierarchical Clustering using complete method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "complete")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 5)
plot(groups)


clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.1519032, higher than Ward method's Dunn's Index with total 5 clusters - 1 overly-dense cluster, 2 relatively-very-sparse clusters and 2 single record clusters - which is not a good clustering.

# Hence Ward method clustering better.

```

Hierarchical Clustering using complete method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "median")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 6)
plot(groups)
clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.1060077, less than Ward method's Dunn's Index with total 6 clusters - 1 overly-dense cluster and 5 single record clusters - which is not a good clustering.

# Hence Ward method clustering better.

```

Hierarchical Clustering using mcquitty method
```{r}

# Since we do not have any informed know how many clusters we need-to build, we prefer to use hierarchical clustring over k-means clustering

hClusters <- hclust(distance, method = "mcquitty")
plot(hClusters)

```
CUTREE into 4 groups
```{r}

groups <- cutree(hClusters, k = 7)
plot(groups)
clusterQuality  = dunn( distance, groups, method = "euclidean")
clusterQuality

# Dunn's Index : 0.1570507, higher than Ward method's Dunn's Index with total 7 clusters - 1 overly-dense cluster, 2 sparse relatively clusters and 4 single record clusters - which is not a good clustering.

# Hence Ward method clustering better.

```
[PART-2] Remove the non-numeric Column
```{r}
# Since grouping by school nameis not relevant removing it

# Remove the school Column. That means, cluster based-on StadiumCapacity, Latitude, Logitude, AthleticRevenue, Endowment and enrollment columns.

colNames = colnames(df)[c(-1, -5, -6)]
df_num = subset.data.frame(x = df, select = colNames, drop = FALSE)
colnames(df_num)

```
```{r}

distance = dist(df_num, method="euclidean")

```

K-means Clustering without normalization
```{r}
# Aim is to organize the records into 10 clusters

# Since, we know the target no of clusters needed, we use k-means clusterig

library(fpc)

kClusters = kmeans(df_num, 10)

plotcluster(df_num, kClusters$cluster)

ggplot(df_num, mapping = aes(x = kClusters$cluster)) +
  geom_histogram() +
  labs(title="No of Records in each cluster", x="Cluster") # + geom_text(aes(label = km$cluster), hjust = 1, vjust = -1) + geom_point()



# aggregate(df_num, by=list(kClusters$cluster),FUN=mean)

# clusterQuality  = dunn( distance, kClusters$cluster, method = "euclidean")

# clusterQuality

# dunn's index: 0.03813315

```

Normalize Data
```{r}
# Normalize the data
df.norm <- data.frame(sapply(X = df_num, FUN = scale))
head(df.norm)

distance  = dist(df.norm, method = 'euclidean')
  
```

K-means Clustering on normalized data
```{r}
# Aim is to organize the records into 10 clusters

# Since, we know the target no of clusters needed, we use k-means clusterig

kClusters = kmeans(df.norm, 10)

plotcluster(df.norm, kClusters$cluster)

aggregate(df_num, by=list(kClusters$cluster),FUN=mean)

clusterQuality  = dunn( distance, kClusters$cluster, method = "euclidean")

clusterQuality

# dunn's index: 0.080144

```

```{r}
```

```{r}
```

```{r}
```